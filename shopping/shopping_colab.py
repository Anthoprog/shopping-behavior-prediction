# -*- coding: utf-8 -*-
"""shopping_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QWTvhT96lymKQc68G3Bfy6F0YoxJOR-P
"""

import csv
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import make_scorer, roc_auc_score, confusion_matrix
from sklearn.model_selection import GridSearchCV
from imblearn.over_sampling import SMOTE
import numpy as np
import warnings
import matplotlib.pyplot as plt
import seaborn as sns

# Define test size
TEST_SIZE = 0.4

def main():
    # Load data from Google Colab's file upload
    from google.colab import files
    uploaded = files.upload()
    filename = list(uploaded.keys())[0]

    # Load data and split into train/test sets
    evidence, labels = load_data(filename)
    X_train, X_test, y_train, y_test = train_test_split(
        evidence, labels, test_size=TEST_SIZE, random_state=42, stratify=labels
    )

    # Apply SMOTE if necessary
    if len(set(y_train)) > 1:
        smote = SMOTE(random_state=42)
        X_train, y_train = smote.fit_resample(X_train, y_train)

    # Standardize the data
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Train model and make predictions
    model = train_model(X_train, y_train)
    predictions = model.predict(X_test)
    sensitivity, specificity = evaluate(y_test, predictions)

    # Print results
    print(f"Correct: {(y_test == predictions).sum()}")
    print(f"Incorrect: {(y_test != predictions).sum()}")
    print(f"True Positive Rate: {100 * sensitivity:.2f}%")
    print(f"True Negative Rate: {100 * specificity:.2f}%")

    # Plot confusion matrix
    plot_confusion_matrix(y_test, predictions, sensitivity, specificity, "Initial Predictions")

def load_data(filename):
    """
    Load shopping data from a CSV file and convert into evidence and labels.
    Clean and validate the data to ensure no missing or invalid values.
    """
    evidence = []
    labels = []

    with open(filename) as file:
        reader = csv.DictReader(file)
        for row in reader:
            try:
                evidence.append([
                    int(row["Administrative"]),
                    float(row["Administrative_Duration"]),
                    int(row["Informational"]),
                    float(row["Informational_Duration"]),
                    int(row["ProductRelated"]),
                    float(row["ProductRelated_Duration"]),
                    float(row["BounceRates"]),
                    float(row["ExitRates"]),
                    float(row["PageValues"]),
                    float(row["SpecialDay"]),
                    month_to_index(row["Month"]),
                    int(row["OperatingSystems"]),
                    int(row["Browser"]),
                    int(row["Region"]),
                    int(row["TrafficType"]),
                    1 if row["VisitorType"] == "Returning_Visitor" else 0,
                    1 if row["Weekend"] == "TRUE" else 0
                ])
                labels.append(1 if row["Revenue"] == "TRUE" else 0)
            except ValueError:
                warnings.warn("Skipping row due to invalid data.")

    return (evidence, labels)

def month_to_index(month):
    """Convert month name to index."""
    months = {
        "January": 0, "February": 1, "March": 2, "April": 3, "May": 4, "June": 5,
        "July": 6, "August": 7, "September": 8, "October": 9, "November": 10, "December": 11
    }
    return months.get(month, -1)

def train_model(evidence, labels):
    """
    Train a k-nearest neighbor model with hyperparameter optimization using StratifiedKFold.
    """
    param_grid = {
        'n_neighbors': [3, 5],
        'weights': ['uniform', 'distance'],
        'metric': ['euclidean', 'manhattan']
    }

    stratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

    model = GridSearchCV(
        KNeighborsClassifier(),
        param_grid,
        scoring=make_scorer(roc_auc_score, needs_proba=True),
        cv=stratified_kfold
    )
    model.fit(evidence, labels)
    print(f"Best Parameters: {model.best_params_}")
    return model.best_estimator_

def evaluate(labels, predictions):
    """
    Evaluate model performance using sensitivity and specificity.
    """
    true_positive = sum(1 for actual, predicted in zip(labels, predictions) if actual == 1 and predicted == 1)
    true_negative = sum(1 for actual, predicted in zip(labels, predictions) if actual == 0 and predicted == 0)
    false_negative = sum(1 for actual, predicted in zip(labels, predictions) if actual == 1 and predicted == 0)
    false_positive = sum(1 for actual, predicted in zip(labels, predictions) if actual == 0 and predicted == 1)

    sensitivity = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0
    specificity = true_negative / (true_negative + false_positive) if (true_negative + false_positive) > 0 else 0

    return (sensitivity, specificity)

def plot_confusion_matrix(y_true, y_pred, sensitivity, specificity, title):
    """
    Plot confusion matrix with sensitivity and specificity.
    """
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Purchased', 'Purchased'], yticklabels=['Not Purchased', 'Purchased'])
    plt.title(f"{title}\nSensitivity: {sensitivity:.2f}, Specificity: {specificity:.2f}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

if __name__ == "__main__":
    main()